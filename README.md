# Generative-AI-Engineering-and-Fine-Tuning-Transformers

Fine-tuning in machine learning is the process of adapting a pretrained model for specific tasks or use cases. During fine-tuning, the collate function tokenizes the dataset, the transformer-based model class defines classification in PyTorch, the forward method applies embeddings to the input, and the train_model function trains a transformer model. 

Fine-tuning enhances efficiency and saves time and computational resources compared to training models from scratch. It helps to transfer learning, time and resource efficiency, tailored responses, and task-specific adaptation.

HuggingFace is an open-source machine learning or ML platform with a built-in transformers library for natural language processing (or NLP) applications. Its built-in datasets can be loaded using the load_dataset function.

**Transformers and Fine Tuning :**
- Loading Models and Inference with Hugging Face Inferences
- [Optional] Pre-training LLMs with Hugging Face
- Pre-Training and Fine-Tuning with PyTorch
- Fine-Tuning Transformers with PyTorch and Hugging Face

**Parameter Efficient Fine Tuning (PEFT) :**
- Adapters with PyTorch
- LoRA with PyTorch
- [Optional] Lab: QLoRA with Hugging Face
