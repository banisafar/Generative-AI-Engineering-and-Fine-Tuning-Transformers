{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-Training and Fine-Tuning with PyTorch**\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "As a machine learning engineer working for a streaming site, your objective is to develop a recommender system based on written movie reviews. Given that the dataset for these reviews is relatively small, your strategy leverages a larger, accessible dataset of magazine articles. The plan is to pretrain the model on this expansive magazine dataset to capture a broad understanding of text and language patterns. Subsequently, the model will undergo fine-tuning on the smaller, specific dataset of movie reviews to adapt its learned features to the particular task of recommending movies.\n",
    "\n",
    "\n",
    "\n",
    "![Documents Overload](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IMDvs-jpgkFICi5fhx6Fr7LBPie55jA.jpg)\n",
    "\n",
    "Fine-tuning a pretrained model can be approached in several ways, each with its advantages and disadvantages:\n",
    "\n",
    "1. **Training the model on the small movie dataset:**\n",
    "   - **Advantages:** Directly training on the movie review data ensures that the model is highly specialized and tailored to the specific characteristics and nuances of the dataset.\n",
    "   - **Disadvantages:** Due to the small size of the dataset, the model is at a higher risk of overfitting. There might not be enough data to effectively capture all of the complexities of language and sentiment in movie reviews.\n",
    "\n",
    "2. **Pretraining the model on a larger general-domain text and fine-tuning the movie dataset for each parameter:**\n",
    "   - **Advantages:** This approach allows every part of the model to learn from the movie reviews, potentially improving its accuracy on the specific task.\n",
    "   - **Disadvantages:** Fine-tuning all parameters with a small dataset can still lead to overfitting. It also requires huge computational resources and time, as adjustments across all parameters might lead to numerous iterations of training.\n",
    "\n",
    "3. **Fine-tuning on the final layer only:**\n",
    "   - **Advantages:** Fine-tuning only the final layer reduces the risk of overfitting, as the bulk of the model retains its pretrained structure. This method is faster and less resource-intensive.\n",
    "   - **Disadvantages:** Because only the final layer is adjusted, the model might not fully adapt to the specifics of the movie reviews, potentially limiting its effectiveness in understanding deeper contextual nuances.\n",
    "\n",
    "Each method offers a balance between adapting to the specifics of the movie review dataset and maintaining the robustness and generalizability learned from the larger magazine article dataset. Choosing the right strategy depends on the specific requirements and constraints of the project, such as available computational resources, the urgency of deployment, and the desired accuracy and specificity of the recommender system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Table of contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Install-required-libraries\">Install required libraries</a></li>\n",
    "            <li><a href=\"#Import-required-libraries\">Import required libraries</a></li>\n",
    "            <li><a href=\"#Define-helper-functions\">Defining helper functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Positional-encodings\">Positional encodings</a></li>\n",
    "    <li><a href=\"#Import-IMDB-dataset\">Import IMDB dataset</a></li>\n",
    "    <ol>\n",
    "        <li><a href=\"#IMDB-dataset-overview\">IMDB dataset overview</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Dataset-composition\">Dataset composition</a></li>\n",
    "            <li><a href=\"#Applications\">Applications</a></li>\n",
    "            <li><a href=\"#Challenges\">Challenges</a></li>\n",
    "            <li><a href=\"#Dataset-splits\">Dataset splits</a></li>\n",
    "            <li><a href=\"#Data-loader\">Data loader</a></li>\n",
    "            <li><a href=\"#Neural-network\">Neural network</a></li>\n",
    "        </ol>\n",
    "    </ol>\n",
    "    <li>\n",
    "        <a href=\"#Training\">Training</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Train-IMDB\">Train IMDB</a></li>\n",
    "            <li><a href=\"#Fine-tune-a-model-pretrained-on-the-AG-News-dataset\">Fine-tune a model pretrained on the AG News dataset</a></li>\n",
    "            <li><a href=\"#Fine-tune-the-final-layer-only\">Fine-tune the final layer only</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Exercise:-Unfreeze-specific-layers-for-fine-tuning\">Exercise: Unfreeze specific layers for fine-tuning</a></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Define and pretrain a transformer-based neural network using PyTorch for a classification task\n",
    "- Fully fine-tune the pretrained model for a different classification task\n",
    "- Compare results by fine-tuning only the last layer of the pretrained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you'll use the following libraries, which are __not__ preinstalled in the Skills Network Labs environment. __You must run the code in the following cell__ to install them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Note__: The package installation step may take a few minutes to complete. Please be patient and wait until the installation finishes before moving on to the next step. Do not interrupt the process or restart the kernel while the installation is in progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.9.0\n",
      "  Downloading matplotlib-3.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting portalocker==2.8.2\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting scikit-learn==1.5.0\n",
      "  Downloading scikit_learn-1.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas~=2.2.2\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy~=1.26.0\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.9.0)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.9.0)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.9.0)\n",
      "  Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (113 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib==3.9.0)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.0) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib==3.9.0)\n",
      "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.9.0)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.0) (2.9.0.post0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.5.0)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.5.0)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.5.0)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas~=2.2.2) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas~=2.2.2)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.17.0)\n",
      "Downloading matplotlib-3.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Downloading scikit_learn-1.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m159.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, threadpoolctl, pyparsing, portalocker, pillow, numpy, kiwisolver, joblib, fonttools, cycler, scipy, pandas, contourpy, scikit-learn, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.0 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.9.0 numpy-1.26.4 pandas-2.2.3 pillow-12.0.0 portalocker-2.8.2 pyparsing-3.2.5 scikit-learn-1.5.0 scipy-1.16.3 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch==2.3.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.3.0%2Bcpu-cp312-cp312-linux_x86_64.whl (190.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.4/190.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchdata==0.9.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchdata-0.9.0%2Bcpu-cp312-cp312-linux_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torchtext==0.18.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchtext-0.18.0%2Bcpu-cp312-cp312-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting filelock (from torch==2.3.0+cpu)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.3.0+cpu) (4.12.2)\n",
      "Collecting sympy (from torch==2.3.0+cpu)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.3.0+cpu)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.3.0+cpu) (3.1.5)\n",
      "Collecting fsspec (from torch==2.3.0+cpu)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.12/site-packages (from torchdata==0.9.0+cpu) (2.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from torchdata==0.9.0+cpu) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from torchtext==0.18.0+cpu) (4.67.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchtext==0.18.0+cpu) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.3.0+cpu) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->torchdata==0.9.0+cpu) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->torchdata==0.9.0+cpu) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->torchdata==0.9.0+cpu) (2024.12.14)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.3.0+cpu)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m140.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchtext, torchdata\n",
      "Successfully installed filelock-3.20.0 fsspec-2025.12.0 mpmath-1.3.0 networkx-3.6.1 sympy-1.14.0 torch-2.3.0+cpu torchdata-0.9.0+cpu torchtext-0.18.0+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 995 ms, sys: 262 ms, total: 1.26 s\n",
      "Wall time: 57.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install matplotlib==3.9.0 portalocker==2.8.2 scikit-learn==1.5.0 pandas~=2.2.2 numpy~=1.26.0\n",
    " \n",
    "%pip install torch==2.3.0+cpu torchdata==0.9.0+cpu torchtext==0.18.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries\n",
    "\n",
    "The following imports the required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/conda/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/conda/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/conda/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "\n",
    "The following are some helper functions to help with plotting, saving, and loading files. These functions are not the main focus of this lab, so you do not have to dwell on these too long. However, do run the cells in this section to define these helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, filename):\n",
    "    \"\"\"\n",
    "    Save a list to a file using pickle serialization.\n",
    "\n",
    "    Parameters:\n",
    "        lst (list): The list to be saved.\n",
    "        filename (str): The name of the file to save the list to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "def load_list_from_file(filename):\n",
    "    \"\"\"\n",
    "    Load a list from a file using pickle deserialization.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the file to load the list from.\n",
    "\n",
    "    Returns:\n",
    "        list: The loaded list.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encodings\n",
    "\n",
    "Positional encodings play a pivotal role in transformers and various sequence-to-sequence models, aiding in conveying critical information regarding the positions or sequencing of elements within a given sequence. To illustrate, let's examine the sentences: \"He painted the car red\" and \"He painted the red car.\" Despite their distinct meanings, it's worth noting that the embeddings for these sentences remain identical in the absence of positional encodings. The following class defines positional encodings by inheriting from PyTorch's `Module` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import IMDB dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')\n",
    "tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))\n",
    "tempdir = tempfile.TemporaryDirectory()\n",
    "tar.extractall(tempdir.name)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB dataset overview\n",
    "\n",
    "The **IMDB dataset** contains movie reviews from the Internet Movie Database (IMDB) and is commonly used for binary sentiment classification tasks. It's a popular dataset for training and testing models in natural language processing (NLP), particularly in the context of sentiment analysis.\n",
    "\n",
    "### Dataset composition\n",
    "\n",
    "- **Reviews**: The dataset consists of 50,000 movie reviews, divided evenly into 25,000 training and 25,000 testing samples.\n",
    "- **Sentiment labels**: Each review is labeled as either positive or negative, indicating the sentiment expressed in the review. The dataset is balanced, with an equal number of positive and negative reviews in both the training and testing sets.\n",
    "- **Text content**: Reviews are presented as plain text and have been preprocessed to some extent. For example, HTML tags are removed, but the text retains its original punctuation and capitalization.\n",
    "- **Usage**: The dataset is commonly used to train models for binary sentiment classification, where the goal is to predict whether a given review is positive or negative based on its text content.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Sentiment analysis**: The primary application of the IMDB dataset is in sentiment analysis, where it serves as a benchmark for various text classification algorithms.\n",
    "- **Natural language processing (NLP)**: The dataset is widely used in NLP research and applications, providing a basis for testing the effectiveness of different models and approaches in understanding human language.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "The dataset is small, so it's hard to train a model from scratch.\n",
    "\n",
    "The following class is defined to traverse the IMDB dataset. The need to define this class arises from the fact that the IMDB dataset is split across a large number of files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        \"\"\"\n",
    "        root_dir: The base directory of the IMDB dataset.\n",
    "        train: A boolean flag indicating whether to use training or test data.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, \"train\" if train else \"test\")\n",
    "        self.neg_files = [os.path.join(self.root_dir, \"neg\", f) for f in os.listdir(os.path.join(self.root_dir, \"neg\")) if f.endswith('.txt')]\n",
    "        self.pos_files = [os.path.join(self.root_dir, \"pos\", f) for f in os.listdir(os.path.join(self.root_dir, \"pos\")) if f.endswith('.txt')]\n",
    "        self.files = self.neg_files + self.pos_files\n",
    "        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)\n",
    "        self.pos_inx=len(self.pos_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return label, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the `IMDBDataset` class previously defined to create iterators for the train and test datasets. In the latter part of the cell, you can return 20 examples from the train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Where to begin? How about with the erroneous synopsis: <br /><br />\"X-Men Origins: Wolverine tells the story of Wolverine\\'s epically violent and romantic past, his complex relationship with Victor Creed, and the ominous Weapon X program.\"<br /><br />His epically violent past turns out to exceptionally non-violent.<br /><br />His relationship with Creed is so glossed over it\\'s difficult to understand how they have any connection at all. We are thrown from one point in the opening scene that shows them as children on the run, to a montage of war scenes that they have fought in throughout their long lifespan, and finally to the present where they are a part of a hardcore government team of assassins.<br /><br />There is nothing by way of showing their relationship as brothers at all. Nothing complex is laid down for us to believe is authentic or even loving. <br /><br />The romantic element of the movie between Silverfox and Wolverine was forced and abrupt. We are thrown into a romance so fast that it\\'s over before you can blink an eye. Having just introduced the character, Silverfox is killed off roughly fifteen minutes later. We are left wondering why we should care about this. Who was she anyway?<br /><br />For a pivotal element of this weak revenge driven story, the romance is surprisingly unexplored. It was rushed in simply because it was required.<br /><br />Oddly enough, when Wolverine finds that his love is dead he leaves her in the woods to rot as he goes off to find Sabertooth. Being the romantic that he is this was out of character for him yet necessary to serve the plot in pulling off a very predictable surprise.<br /><br />As for the weapon X program, lets just say that after the painfully crippling procedure Wolverine is up and running. Eventually he arrives at the home of a conveniently old yet overwhelmingly loving couple. Surprisingly Ma and Pa Kent aren\\'t alarmed when finding a naked sweaty man in their barn. Is it any wonder what fate awaits them?<br /><br />In the previous films and the comic books, the main reason that Wolverines\\' amnesia plagued him partly hinged on the fact that he was said to have been viciously evil and coldblooded.<br /><br />Knowing this was the case...did he really want to remember such horrors or keep them hidden and continue his current more positive lifestyle of fighting against the villains of the world alongside his team mates?<br /><br />As hinted to in X2: X-men United when Stryker gives up some of his secrets it is said that Wolverine would be disturbed if he had known of the evil works they committed together. This film sets up the team fairly well only they don\\'t really do much of anything. No disturbing violence, no ruthless actions, they merely harass a few natives in foreign lands for the ten or fifteen minutes they are on screen. <br /><br />It seems that Wolverine wasn\\'t an evil man under Stryker at all. Instead he was constantly trying to put a leash on his brother Sabertooth which consequently WAS the violent agent we all thought Wolverine was. Eventually he just leaves all together.<br /><br />No conflict of duality here at all.<br /><br />Idiotically REMOVING that character conflict of good and evil DULLED the story immensely. They may as well have given him rubber claws.<br /><br />There were a ton of other errors in this film that contradicted the X-Men trilogy, including the introduction of one of the lamest Deus Ex Machinas to ever hit a script.<br /><br />Magic memory-erasing bullets. <br /><br />Really?<br /><br />Apparently they are the only thing to bring down Wolverine. Yet this was apparently forgotten when agent after agent was sent to bring him down with bullets and bombs that would surely not work on him at all.<br /><br />Another problem with this film is that it tried to focus on Wolverine while throwing in a ton of other mutants which did little to nothing at all. Interesting characters were mere window dressing and did nothing for the story. Most were in the film for 5-10 minutes max and yet you find yourself wishing we saw more of them and less of Wolverine.<br /><br />Fred Dukes (the Blob but not the comic version) can punch a launched tank missile with little to no physical damage to him at all, but a simple headbutt from Wolverines metal noggin is enough to daze him?<br /><br />Cyclops optic beams (which instead of being concussive force are now more akin to lasers) can burn through buildings but when fired at Sabertooth directly it simply smashes him into the ground without even damaging his clothes. Adamantium trench coats anyone?<br /><br />The (gravity defying) mutant Gambit, instead of utilizing his signature cards, is made into some sort of crazy acrobat. In one poorly edited scene he is knocked unconscious by Wolverine...then amazingly enough a few minutes later he is on a rooftop running TOWARDS Wolverine. How he regained consciousness, ran away a few blocks, climbed up a building, then ran back to Wolverine and Sabertooth in the middle of a scratching match is a mystery yet to be explained.<br /><br />Some have excused this films weakness by claiming it was made from a comic and therefore should be weak on character and heavy on flash. The idea that this movie being a comic film is flimsy and superficial because of that fact is incorrect.<br /><br />The comic book source material, the REAL origin of Wolverine...is a story worth bringing to the screen. It doesn\\'t sugar coat his past nor treat the reader like mindless CGI junkies. It is a well crafted story and although retold and readjusted over time, began with WEAPON X by Barry Windsor-Smith. A much more intense and exciting story.<br /><br />This FOX film should seriously be forgotten. <br /><br />Anyone have that magic gun?<br /><br />4/10')\n",
      "(0, '***Possible Spoilers***<br /><br />When I saw this today I had some expectation of how it would be like, not too high, but not low either. This was nothing like I expected at all though, it seems to me like the movie makers couldn\\'t make up their mind of what kind of movie to make.<br /><br />In the begining of the film it\\'s somewhat mysterious and kinda exciting, but that\\'ll soon change to some ridiculous scenes - very obvious scenes... As I watched further I almost fell asleep a couple of times.<br /><br />The ending is the most ridiculous of all though, almost splatter/comedy...<br /><br />I\\'m not saying it doesn\\'t have some good scenes it\\'s just that the film never becomes \"whole\".<br /><br />4/10 Movie-Man')\n",
      "(0, 'OK this movie was really \"unique\" shall we say. Carrie Fisher was by far the most talented in the movie, even though it is said she had a coke problem during the movie. but she said that she can\\'t remember to much of it. well thats her excuse but whats everyone else\\'s??? i can\\'t imagine they all had a coke problem.whatever it wasn\\'t all that bad i mean i guess the plot was OK and it had some pretty funny moments. although the part where the guy gets that thing shoved in his head was a little too violent for my taste. oh well i guess i was disappointed cause Carrie fisher is like so awesome and this movie did nothing for her.')\n",
      "(0, \"The title should have been the walker. The guy expend 90% of the movie walking. He doesn't know what he wants, or what he is. Go through life stealing peoples identity for nothing. He gets no benefit, no money, nothing pretending to be another person.<br /><br />No body was able to understand why he was pretending to be somebody else.<br /><br />The only thing that was clear in this movie is that he love his father and was a good son. But the rest was crap.<br /><br />May be director is a looser that would like to be somebody else. But what he really should do is to get a real job, because after his movie, I don't think he has a chance to make as a movie producer.\")\n",
      "(0, 'This little show is obviously some stupid little prequel/spin off of the original series.<br /><br />Compared to the live action series this show is utter crap. The live action show had intelligent jokes and story lines. While the animated series is basically a toned down bittersweet version for younger viewers to digest but i think maybe kids deteste this crap.<br /><br />The storyline in every episode is basically just Sabrina has some stupid and pointless dillemma and she uses magic to fix it. Thats basiclly the idea every episode. The most bizarre episode was when Sabrina uses magic to become Gem and Gem to become Sabrina. So then Gem becomes a witch and hypnotizes harvery to become her slave. This then leads to a bizarre yet rather interesting scene were Gem says \"just adore me for now\" and harvey get down on his hands and knees and starts kissing her feet like shes a god. (which is quite right since he\\'s her mind control slave) But this stupid spin-off is not worth the time or the effort.')\n",
      "(0, \"I felt brain dead, I'll tell you. This is the worst film I have ever bought. (in my ignorance I thought this was the Peter Jackson film of the same name). The performances are so terrible they are laughable. The special effects have not stood the test of time and look dire. The script promotes that kind of TV movie, stare into the middle distance kind of acting. The cast look as if they have been taking lessons from Joey Tribbiani, they have one look each, and stick to it. Plus I have never been confused by a movie until I sat down to watch this. The is it a dream or no plot is so terrible that frustration sets in within a few minutes. Avoid like a plague.\")\n",
      "(0, \"please don't rent or even think about buying this movie.they don't even have it available at the red box to rent which would cost a $1 & i think its worth less than that.the main reason why i rented this d movie was because Jenna Jameson is in the movie lol between 2-5 min.i will give credit that the movie had hot chicks and quite a bit of nudity but other than that you might as well buy another d horror movie that has the same thing with nobody you know.Ginger Lynn has more acting time in this movie than Jenna & she's not even on the front cover of the movie nor her name.i recommend people to watch zombie strippers because you see Jenna almost throughout the whole movie & nude most of the time.this movie is a big disappointment & such a huge waste of time.\")\n",
      "(0, '\"Sleepwalkers\" is the first film which Stephen King has written a script for. Given this, and the excellent Santo & Johnny song that they used as the theme of the movie, you would be expecting a odd, and ultimately fulfilling viewing experience. Unfortunately, that\\'s not what you\\'d be getting. The thing is, they could have probably made it a good movie. The beginning is intriguing what with it\\'s small town spooky atmosphere. But something strange happens about 20 minutes into the film. The film turns funny for no apparent reason! From that moment on the whole atmosphere of \"Sleepwalkers\" is ruined.For those of you who have seen it, who can ever forget good old Johnny screaming out \"COP KABOB!!\" after jabbing the pencil into that one cop\\'s ear?!? But don\\'t get me wrong, the humor has no redeemiing quality. I just rented it again to see if mabye I was wrong the first time around, given how original the plot sounded, but I was right. Man, what a waste. I can\\'t believe they got the rights to that Santo & Johnny song. I gave this a 2.')\n",
      "(0, \"Cuba Gooding Jr. is a secret service agent who blames himself over the assassination of the U.S. President, i'll point out straight away that this is not the type of role that this very talented actor is noted for, and this film shows us why. He teams up with a persistent news reporter (Angie Harmon) to uncover the conspiracy surrounding the president's death, and so on, blah, blah, blah.<br /><br />Even with a cast of James Woods, Cuba Gooding Jr, Anne Archer and Angie Harmon 'End Game' fails to grab your attention, plain and simple; some of the action is good, the acting isn't all bad and the story although clichéd and done before could have lead to an entertaining and enjoyable movie - WELL IT DOESN'T! The writing of the script and the direction makes absolutely sure of that, at no point does it suck you into the story or make you give the slightest thought to any of the characters.<br /><br />4/10 It's Boring, Predictable and Dull.\")\n",
      "(0, \"I saw this movie on Mystery Science Theater 300. It sucked so much. If I hadn't been watching it on MST3K, I probably would've thrown it out the window. The characters were incredibly lame and it didn't provide much of a plot in my opinion.\")\n",
      "(1, '... for Paris is a moveable feast.\" Ernest Hemingway<br /><br />It is impossible to count how many great talents have immortalized Paris in paintings, novels, songs, poems, short but unforgettable quotes, and yes - movies. The celebrated film director Max Ophüls said about Paris, <br /><br />\"It offered the shining wet boulevards under the street lights, breakfast in Montmartre with cognac in your glass, coffee and lukewarm brioche, gigolos and prostitutes at night. Everyone in the world has two fatherlands: his own and Paris.\" <br /><br />Paris is always associated with love and romance, and \"Paris, Je T\\'Aime\" which is subtitled \"Petite romances,\" is a collection of short films, often sketches from 18 talented directors from all over the world. In each, we become familiar with one of the City of Light 20 arrondissements and with the Parisians of all ages, genders, colors, and backgrounds who all deal in love in its many variations and stages. In some of the \"petite romances\" we are the witnesses of the unexpected encounters of the strangers that lead to instant interest, closeness, and perhaps relationship: like for Podalydès and Florence Muller in the street of Montmartre in the opening film or for Cyril Descours and Leïla Bekhti as a white boy and a Muslim girl whose cross-cultural romance directed by Gurinder Chadha begins on Quais de Seine. I would include into this category the humorous short film by Gus Van Sant. In \"Le Marais\" one boy pours his heart out to another boy confessing of sudden unexpected closeness, asking permission to call - never realizing that the object of his interest does not understand French.<br /><br />Some of the vignettes are poignant and even dark. In Walter Salles and Daniela Thomas\\' Loin du 16ème, Catalina Sandino Mareno (amazing Oscar nominated debut for Maria full of Grace) is single, working-class mother who has to work as a nanny in a wealthy neighborhood to pay for daycare where she drops her baby every morning before she goes to work. One of most memorable and truly heartbreaking films is \"Place des Fêtes\" by Oliver Schmitz. Aïssa Maïga and Seydou Boro co-star as two young people for who love could have happened. There were the promises of it but it was cut short due to hatred and intolerance that are present everywhere, and the City of Love and Light is no exception. Another one that really got to me was \"Bastille\", written and directed by Isabel Coixet, starring Sergio Castellitto, Miranda Richardson, and Leonor Watling. Castellitto has fallen out of love with his wife, Richardson but when he is ready to leave with the beautiful mistress, the devastating news from his wife\\'s doctor arrives...<br /><br />I can go on reflecting on all 18 small gems. I like some of them very much. The others felt weak and perhaps will be forgotten soon but overall, I am very glad that I bought the DVD and I know that I will return to my favorite films again and again. They are \"Place des Fêtes\" that I\\'ve mentioned already, \"Père-Lachaise\" directed by Wes Craven that involves the ghost of one of the wittiest and cleverest men ever, Oscar Wilde (Alexander Payne, the director of \"Sideways\") who would save one troubled relationship. Payne also directed \"14th Arrondissement\" in which a lonely middle-aged post-worker from Denver, CO explores the city on her own providing the voice over in French with the heavy accent. Payne\\'s entry is one of the most moving and along with hilarious \"Tuileries\" by Joel and Ethan Coen with (who else? :)) Steve Buschemi is my absolute favorite. In both shorts, American tourists sit on the benches (Margo in the park, and Steve in Paris Metro after visiting Louvers) observing the life around them with the different results. While Margo may say, \"My feeling\\'s sad and light; my sorrow is bright...\" Steve\\'s character will find out that sometimes, even the most comprehensive and useful tourist guide would not help a tourist avoiding doing the wrong things in a foreign country.')\n",
      "(1, \"This was a pretty decent movie. This movie is good to just sit down and watch and be entertained. Just a typical Hollywood film. This movie will never win an Oscar or anything and definitely doesn't deserve one, but I thought it was pretty good. It's kind of like the show 24 but set into movie format. If you like the whole we've got to stop the terrorist from killing the president kind of movie then you will enjoy this flick. I personally think that storyline has been done WAY too much, but The Sentinel does add a little twist with the mole in the Secret Service. All in all, this movie won't leave your jaw to the floor or change your life, but who says every single movie has to be like that to be good?\")\n",
      "(1, \"Don't waste time reading my review. Go out and see this astonishingly good episode, which may very well be the best Columbo ever written! Ruth Gordon is perfectly cast as the scheming yet charming mystery writer who murders her son-in-law to avenge his murder of her daughter. Columbo is his usual rumpled, befuddled and far-cleverer-than-he-seems self, and this particular installment features fantastic chemistry between Gordon and Falk. Ironically, this was not written by heralded creators Levinson or Link yet is possibly the densest, most thoroughly original and twist-laden Columbo plot ever. Utterly satisfying in nearly every department and overflowing with droll and witty dialogue and thinking. Truly unexpected and inventive climax tops all. 10/10...seek this one out on Netflix!\")\n",
      "(1, '\"Spin it!\"<br /><br />The 90s opened up with a clever Disney favorite, \"TaleSpin,\" the TV cartoon series that featured characters from \"The Jungle Book.\" Join Baloo and Kit Cloudkicker as they fly the Sea Duck like you\\'ve never seen it before: out of Cape Suzette, to Louie\\'s, up mountains, through jungles, on water, in volcanoes, looking for adventure, looking for treasure, looking for fun, all in one action-packed cartoon adventure!!!!!<br /><br />This was a favorite of mine as well as my family\\'s. This ran on The Disney Afternoon the entire first half of the 90s until the original cartoons moved to the Old Disney Channel in 1995, which I have seen on vacation once in 1996 before getting cable in March 1997.<br /><br />And good news: today the DVDs are here!!!!! Relive the fun and excitement of \"Dun, dun, dun, TaleSpin!!!!!\"<br /><br />10/10')\n",
      "(1, 'A delightful piece of cinema storytelling in a simple but effective way. Cinema after all is a visual media and Igor used its full potential. A young restless man boards a train with no destination in mind. In one of the compartments he meets with a girl. Words are not exchanged but their laundry washing are and from there we are taken on a ride with other peculiar characters and situations. The two leads are perfectly cast as their unique features tell you a story that needs no words.')\n",
      "(1, 'Note that I did not say that it is better...just more enjoyable. The lack of social commentary and realism helps keep things moving.<br /><br />I was actually sort of surprised that this was not a Troma movie, as it has all the Troma trademarks, including spewing acidic liquids, wisecracks from the villain after every murder, a ridiculous bathtub rape scene (which is sort of hard to get upset about, since the rapist is a snowman), and dumb deputies.<br /><br />There\\'s a lot to love: <br /><br />1. A snowman, about whom it is remarked that he has no legs or feet, drives a police cruiser around town.<br /><br />2. Even though it is supposed to be close to (or below) freezing, nobody\\'s breath shows and there are no signs of car exhaust when cars are running.<br /><br />3. The snow reminds you more of flocking or Styrofoam peanuts than actual snow.<br /><br />4. A teenage girl gets the hots for her boyfriend just a few hours after her brother is gruesomely murdered. She talks him into breaking into the sheriff\\'s house, of all places, in order to get it on. But first, she tells him that he has to build a nice fire in the fireplace and open some wine.<br /><br />5. After teen-aged Jake\\'s head is cut off by a sled runner, his father argues with the sheriff about whether Tommy, the sheriff\\'s son, had anything to do with it. The sheriff maintains that Tommy wouldn\\'t have been fighting with Jake because Jake \"is at least two feet taller than Tommy.\" At that moment, someone in the background chimes in, \"Not anymore!\" <br /><br />6. When the evil snowman finally starts to melt away, the sheriff wrestles with a flat snowman made out of some sort of fabric for an extended period. This is much better than Tarzan wrestling with rubber crocodiles or gladiators wrestling with stuffed lions. If I were an actor, I would not have been able to keep a straight face at this point.<br /><br />All in all, a fun film. There\\'s not really even that much blood.')\n",
      "(1, 'Out of these Pokemon films (which are in order of best to least for me): Pokemon The First Movie, Pokemon 4Ever, Pokemon Heroes, Pokemon 200 and Pokemon: Entei and the Unknown, this is probably the one most concerning the environment, arguably the most beautiful and the most calming one. Whether these are good points for you or not, \"Pokemon 4Ever,\" still has entertained many.<br /><br />As well as the three points covered above, this pokemon film includes good humour and good CGI (as well as anime). The time travelling theme of the film is represented in a good way and Team Rocket (the comic reliefs/rubbish baddies) end up with quite good gags and end up being more main characters than sidekicks. <br /><br />The flaws are, as always, the rather unnecessary violence and action and the baddie is pretty uninteresting, even more so than a few Pokemon film baddies. <br /><br />A strange pokemon is being tracked down by a pokemon hunter in a forest. A young boy tries to save the pokemon and it takes him somewhere...<br /><br />Meanwhile, Ash, Brock and Misty are entering a large forest...<br /><br />Curious? Watch the rest...<br /><br />Good for all Pokemon fans and \"American\" anime movie fans, enjoy \"Pokemon 4Ever\"! :-)')\n",
      "(1, 'This movie rocks\" Jen sexy as ever and Polly wow were we really ever that young this movie can still touch the hearts of a lot of teens it needs to be put on DVD soon or it will become a classic. i Really enjoyed growing up to this movie i have always had a crush on Jen now i am too old but to this movie is made for all gens> you know i come from the early 80,s area were i Had to watch everyone els live the life i wanted but thru movies i can do that all over again i guess in short i am hoping and wishing that this movie not be lost in time but reborn to the youth so they may enjoy the heart warm filling you get learning about hormones and datting problems and how to get away with stuff that seems so major back then but don\\'t mean nothan now so this movie is a dating tool.')\n",
      "(1, \"Though structured totally different from the book by Tim Krabbé who wrote the original 'The Vanishing' (Spoorloos) it does have the same overall feel, except for that Koolhoven's style is less business-like and more lyric. The beginning is great, the middle is fine, but the sting is in the end. A surprise emotional ending. As you could read in several magazines there is some sex in the film, but it is done all very beautifully. Never explicit, but with lots of warmth and sometimes even humour. It is a shame American films can't be as open an honoust as this one. Where Dutch films tend to go just over the edge when it comes to this subject, 'De Grot' stays always within the boundaries of good taste. 'De Grot' tells an amazing story stretched over more than 30 years. When you'll leave the cinema you'll be moved. What can we ask more of a film? Anyway, this film even gives more....\")\n",
      "(1, \"Excellent. Gritty and true portrayal of pioneer ranch life on the Western plains with an emphasis on the woman's role and place. A moving film, lovingly made, and based on real people and their actual experiences. Low budget, independent film; never made any money. Definitely not the romanticized, unrealistic Hollywood version of pioneer life.\")\n"
     ]
    }
   ],
   "source": [
    "root_dir = tempdir.name + '/' + 'imdb_dataset'\n",
    "train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data\n",
    "test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test data\n",
    "\n",
    "start=train_iter.pos_inx\n",
    "for i in range(-10,10):\n",
    "    print(train_iter[start+i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines the mapping of numeric labels to positive and negative reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_label = {0: \" negative review\", 1: \"positive review\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code checks to ensure that there are exactly two classes in the train dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads a basic English tokenizer and defines a function called ```yield_tokens``` that uses the tokenizer to break down text data yielded by an iterator into tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"Yield tokens for each data sample.\"\"\"\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following code loads a pretrained word embedding model called GloVe into a variable called `glove_embedding`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove-6B.zip: 134MB [00:07, 19.0MB/s]                              \n",
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25472.42it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note that GloVe embeddings are typically downloaded using:\n",
    "#glove_embedding = GloVe(name=\"6B\", dim=100)\n",
    "# However, the GloVe server is frequently down. The code below offers a workaround\n",
    "\n",
    "\n",
    "class GloVe_override(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        #name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "class GloVe_override2(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        #name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override2, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "try:\n",
    "    glove_embedding = GloVe_override(name=\"6B\", dim=100)\n",
    "except:\n",
    "    try:\n",
    "        glove_embedding = GloVe_override2(name=\"6B\", dim=100)\n",
    "    except:\n",
    "        glove_embedding = GloVe(name=\"6B\", dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code builds a vocabulary object from a pretrained GloVe word embedding model and sets the default index to the <unk> token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe,vocab\n",
    "# Build vocab from glove_vectors\n",
    "vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5452"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2928"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6452e-02, -1.0569e+00,  3.3078e-01, -3.7137e-01, -5.1399e-01,\n",
       "         9.1245e-01,  1.5547e-01,  3.3098e-01, -6.8504e-01, -1.8018e-01,\n",
       "         1.3459e+00,  9.3845e-01, -2.9332e-01, -8.6569e-01,  1.2519e-01,\n",
       "        -3.5832e-01, -3.1904e-01,  2.7425e-01,  1.2708e-01, -4.8572e-01,\n",
       "         1.1603e+00,  2.5167e-01,  4.4637e-01,  2.2615e-01,  7.8303e-01,\n",
       "        -1.3101e-02, -9.1040e-01,  4.5090e-02,  1.0443e+00, -5.0086e-01,\n",
       "        -1.2350e+00,  1.4729e-01,  1.0577e+00,  3.7815e-01,  1.0333e-01,\n",
       "         5.5118e-01, -1.7615e-01,  4.5555e-01,  3.9198e-01,  7.0589e-01,\n",
       "        -2.9854e-01, -4.8874e-01,  1.0345e+00, -1.2466e-01, -1.0306e+00,\n",
       "         2.7675e-01,  2.3323e-01,  5.4984e-02,  3.0979e-02, -1.2951e+00,\n",
       "        -5.1821e-01, -4.4341e-01, -1.2075e+00,  1.2806e+00,  2.8087e-01,\n",
       "        -1.4001e+00, -1.0208e+00,  1.6185e+00,  9.0042e-01, -8.4780e-02,\n",
       "        -9.1359e-02,  1.5204e-01,  6.6034e-01,  1.1221e+00, -1.5070e-02,\n",
       "         1.4552e-01, -1.1626e-01, -6.9693e-01, -7.4633e-02, -4.9922e-01,\n",
       "        -1.2102e+00,  7.7924e-01,  1.0868e-01,  4.3874e-01,  3.5213e-01,\n",
       "         9.1757e-01, -6.1687e-01, -7.9719e-01,  6.7132e-01, -3.7664e-01,\n",
       "         3.4417e-01, -3.4068e-01,  1.0924e-01,  2.3998e-01, -2.8555e-01,\n",
       "        -1.3474e-01,  2.7644e-01, -8.5771e-01,  2.7368e-01,  8.5454e-02,\n",
       "        -6.4732e-01, -2.1162e-02, -4.2819e-01,  7.8247e-01,  4.5627e-02,\n",
       "         4.0780e-01, -5.1621e-04, -2.8867e-01,  5.6548e-01, -8.3285e-02])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.vectors[5452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vehicle', 0.8630838990211487), ('truck', 0.8597878813743591), ('cars', 0.8371669054031372), ('driver', 0.8185910582542419), ('driving', 0.7812636494636536), ('motorcycle', 0.7553157210350037), ('vehicles', 0.7462257146835327), ('parked', 0.7459464073181152), ('bus', 0.737270712852478), ('taxi', 0.7155269980430603)]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def topk_similar(word, k=10):\n",
    "    idx = glove_embedding.stoi.get(word, None)\n",
    "    if idx is None:\n",
    "        raise ValueError(f\"'{word}' not in GloVe vocab\")\n",
    "\n",
    "    vecs = glove_embedding.vectors                      # (V, 100)\n",
    "    q = vecs[idx]                                       # (100,)\n",
    "    vecs_norm = F.normalize(vecs, p=2, dim=1)           # (V, 100)\n",
    "    q_norm = F.normalize(q, p=2, dim=0)                 # (100,)\n",
    "\n",
    "    sims = torch.mv(vecs_norm, q_norm)                  # (V,)\n",
    "    vals, inds = torch.topk(sims, k + 1)                # +1 includes the word itself\n",
    "\n",
    "    # Convert indices back to tokens (torchtext vocab has get_itos())\n",
    "    itos = glove_embedding.itos if hasattr(glove_embedding, \"itos\") else None\n",
    "    if itos is None:\n",
    "        # If you built your own vocab object, use vocab.get_itos()\n",
    "        itos = vocab.get_itos()\n",
    "\n",
    "    results = []\n",
    "    for v, i in zip(vals.tolist(), inds.tolist()):\n",
    "        token = itos[i]\n",
    "        if token != word:\n",
    "            results.append((token, v))\n",
    "        if len(results) == k:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "print(topk_similar(\"car\", k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 0.8798074722290039), ('rabbit', 0.7424426674842834), ('cats', 0.7323004603385925), ('monkey', 0.7288710474967957), ('pet', 0.7190141081809998), ('dogs', 0.7163872718811035), ('mouse', 0.6915251016616821), ('puppy', 0.6800068020820618), ('rat', 0.6641026735305786), ('spider', 0.6501135230064392)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(topk_similar(\"cat\", k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of words in the vocab:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the ```vocab``` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(['he'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following converts the dataset into map-style datasets and then performs a random split to create separate training and validation datasets. The training dataset will contain 95% of the samples in the original training set, while the validation dataset will contain the remaining 5%. These datasets can be used for training and evaluating a machine learning model for text classification on the IMDB dataset. The final performance of the model will be evaluated on the hold-out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing iterators to map-style datasets.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determine the number of samples to be used for training and validation (5% for validation).\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
    "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that the Skills Network currently does not offer GPU access to learners. As a result, training on the full dataset could be time-consuming. To address this, you further reduce the size of the training set. This approach helps you mimic the training process as if a GPU were available. However, if you want to train using the full IMDB dataset, you must either comment out or remove the two lines in the upcoming code block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(len(train_dataset) * 0.05)\n",
    "split_train_, _ = random_split(split_train_, [num_train, len(split_train_) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code checks to see if a CUDA-compatible GPU is available in the system using PyTorch, a popular deep learning framework. If a GPU is available, it assigns the device variable to \"cuda\" (which stands for CUDA, the parallel computing platform and application programming interface model developed by NVIDIA). If a GPU is not available, it assigns the device variable to \"cpu\" (which means the code will run on the CPU instead).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prepares the text processing pipeline with the tokenizer and vocabulary. The text pipeline is used to process the raw data strings from the dataset iterators.\n",
    "\n",
    "The function **```text_pipeline```** first tokenizes the input text, then **```vocab```** is applied to get the token indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the **`collate_fn`** function is used in conjunction with data loaders to customize the way batches are created from individual samples. The provided code defines a `collate_batch` function in PyTorch, which is used with data loaders to customize batch creation from individual samples. It processes a batch of data, including labels and text sequences. It applies the `text_pipeline` function to preprocess the text. The processed data is then converted into PyTorch tensors and returned as a tuple containing the label tensor, text tensor, and offsets tensor representing the starting positions of each text sequence in the combined tensor. The function also ensures that the returned tensors are moved to the specified device (for example, GPU) for efficient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "\n",
    "        label_list.append(_label)\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert the dataset objects to data loaders by applying the `collate` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see what these data loaders generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,seqence=next(iter(valid_dataloader))\n",
    "label,seqence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "Neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a class called Net that represents a text classifier based on a PyTorch TransformerEncoder.\n",
    "The constructor takes the following arguments:\n",
    "\n",
    "- `num_class`: The number of classes to classify\n",
    "- `vocab_size`: The size of the vocabulary\n",
    "- `freeze`: Whether to freeze the embedding layer\n",
    "- `nhead`: The number of heads in the transformer encoder\n",
    "- `dim_feedforward`: The dimension of the feedforward layer in the transformer encoder\n",
    "- `num_layers`: The number of transformer encoder layers\n",
    "- `dropout`: The dropout rate\n",
    "- `activation`: The activation function to use in the transformer encoder\n",
    "- `classifier_dropout`: The dropout rate for the classifier\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "- `emb`: An embedding layer that maps each word in the vocabulary to a dense vector representation\n",
    "- `pos_encoder`: A positional encoding layer that adds positional information to the word vectors\n",
    "- `transformer_encoder`: A transformer encoder layer that processes the sequence of word vectors and extracts high-level features\n",
    "- `classifier`: A linear layer that maps the output of the transformer encoder to the desired number of classes\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "\n",
    "        self,\n",
    "        num_class,\n",
    "        vocab_size,\n",
    "        freeze=True,\n",
    "        nhead=2,\n",
    "        dim_feedforward=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #self.emb = embedding=nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n",
    "        self.emb = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n",
    "        embedding_dim = self.emb.embedding_dim\n",
    "\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_dim, num_class)\n",
    "        self.d_model = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can then be trained on labeled data from the IMDB dataset with two classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net(num_class=2,vocab_size=vocab_size).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following **`predict`** function takes in a text, a text pipeline, and a model as inputs. It uses a pretrained model passed as a parameter to predict the label of the text for text classification on the IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline, model):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "        model.to(device)\n",
    "        output = model(text)\n",
    "        return imdb_label[output.argmax(1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"I like sports and stuff\", text_pipeline, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a function to evaluate the model's accuracy on a dataset. Here, you define two nearly identical evaluation functions, one that provides a `tqdm` progress bar, and one that does not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model_eval):\n",
    "    model_eval.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(dataloader):\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            output = model_eval(text)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            total_acc += (predicted == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_no_tqdm(dataloader, model_eval):\n",
    "    model_eval.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            output = model_eval(text)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            total_acc += (predicted == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code evaluates the performance of your model. Note that this can take approximately 4 minutes on a CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the current performance of the model is no better than average. This outcome is expected, considering that the model has not undergone any training yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines the training function used to train your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader,  epochs=1000, save_dir=\"\", file_name=None):\n",
    "    cum_loss_list = []\n",
    "    acc_epoch = []\n",
    "    acc_old = 0\n",
    "    model_path = os.path.join(save_dir, file_name)\n",
    "    acc_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + \"_acc\")\n",
    "    loss_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + \"_loss\")\n",
    "    time_start = time.time()\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.train()\n",
    "        #print(model)\n",
    "        #for parm in model.parameters():\n",
    "        #    print(parm.requires_grad)\n",
    "        \n",
    "        cum_loss = 0\n",
    "        for idx, (label, text) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            label, text = label.to(device), text.to(device)\n",
    "\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            loss.backward()\n",
    "            #print(loss)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            cum_loss += loss.item()\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {cum_loss}\")\n",
    "\n",
    "        cum_loss_list.append(cum_loss)\n",
    "        accu_val = evaluate_no_tqdm(valid_dataloader,model)\n",
    "        acc_epoch.append(accu_val)\n",
    "\n",
    "        if model_path and accu_val > acc_old:\n",
    "            print(accu_val)\n",
    "            acc_old = accu_val\n",
    "            if save_dir is not None:\n",
    "                pass\n",
    "                #print(\"save model epoch\",epoch)\n",
    "                #torch.save(model.state_dict(), model_path)\n",
    "                #save_list_to_file(lst=acc_epoch, filename=acc_dir)\n",
    "                #save_list_to_file(lst=cum_loss_list, filename=loss_dir)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print(f\"Training time: {time_end - time_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train IMDB\n",
    "\n",
    "The following code sets the learning rate (LR) to 1, which determines the step size at which the optimizer updates the model's parameters during training. The CrossEntropyLoss criterion is used to calculate the loss between the model's predicted outputs and the ground truth labels. This loss function is commonly employed for multiclass classification tasks.\n",
    "\n",
    "The chosen optimizer is Stochastic Gradient Descent (SGD), which optimizes the model's parameters based on the computed gradients with respect to the loss function. The SGD optimizer uses the specified learning rate to control the size of the weight updates.\n",
    "\n",
    "Additionally, a learning rate scheduler is defined using StepLR. This scheduler adjusts the learning rate during training, reducing it by a factor (gamma) of 0.1 after every epoch (step) to improve convergence and fine-tune the model's performance. These components together form the essential setup for training a neural network using the specified learning rate, loss criterion, optimizer, and learning rate scheduler.\n",
    "\n",
    "For the sake of time efficiency, the number of epochs has been set to 2. This is to give you a practical demonstration of what the training process looks like. However, if you were to train this model in a real-world scenario, you would likely increase the number of epochs to a larger figure, such as 100 or more. Given the reduced training set defined earlier, it takes approximately 2 minutes to complete 2 epochs of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_IMDB dataset small2.pth\"\n",
    "train_model(model=model, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion, \n",
    "            train_dataloader=train_dataloader, \n",
    "            valid_dataloader=valid_dataloader, \n",
    "            epochs=2, \n",
    "            save_dir=save_dir, \n",
    "            file_name=file_name\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the model you trained earlier with just 2 epochs and a reduced dataset won’t be used. Instead, you’ll use a model that has been pretrained using the same method but on the full dataset and with 100 epochs.\n",
    "\n",
    "The following code plots the cost and validation data accuracy for each epoch of the pretrained model up to and including the epoch that yielded the highest accuracy. As you can see, the pretrained model achieved an accuracy of over 85% on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/sybqacL5p1qeEO8d4xRZNg/model-IMDB%20dataset%20small2-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eOt6woGoaOB565T0RLH5WA/model-IMDB%20dataset%20small2-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads your pretrained model and evaluates its performance on the test set. This can take approximately 4 minutes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/q66IH6a7lglkZ4haM6hB1w/model-IMDB%20dataset%20small2.pth')\n",
    "model_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "evaluate(test_dataloader, model_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the pretrained model achieved an accuracy of approximately 83% on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune a model pretrained on the AG News dataset\n",
    "\n",
    "Rather than training a model on the IMDB dataset as you did earlier, you have the option to fine-tune a model that has been pretrained on the AG News dataset, which is a collection of news articles. The goal of the AG News dataset is to categorize news articles into one of four categories: Sports, Business, Sci/tech, or World. You’ll start training a model from scratch on the AG News dataset. To save time, you can do this in just one cell. Also, for the sake of efficiency, train for only 2 epochs on a smaller dataset to demonstrate what the training process looks like. Note that this can take approximately 3 minutes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_ag_news = AG_NEWS(split=\"train\")\n",
    "\n",
    "num_class_ag_news = len(set([label for (label, text) in train_iter_ag_news ]))\n",
    "num_class_ag_news\n",
    "\n",
    "# Split the dataset into training and testing iterators.\n",
    "train_iter_ag_news, test_iter_ag_news = AG_NEWS()\n",
    "\n",
    "# Convert the training and testing iterators to map-style datasets.\n",
    "train_dataset_ag_news = to_map_style_dataset(train_iter_ag_news)\n",
    "test_dataset_ag_news = to_map_style_dataset(test_iter_ag_news)\n",
    "\n",
    "# Determine the number of samples to be used for training and validation (5% for validation).\n",
    "num_train_ag_news = int(len(train_dataset_ag_news) * 0.95)\n",
    "\n",
    "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
    "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
    "split_train_ag_news_, split_valid_ag_news_ = random_split(train_dataset_ag_news, [num_train_ag_news, len(train_dataset_ag_news) - num_train_ag_news])\n",
    "\n",
    "# Make the training set smaller to allow it to run fast as an example.\n",
    "# IF YOU WANT TO TRAIN ON THE AG_NEWS DATASET, COMMENT OUT THE 2 LINEs BELOW.\n",
    "# HOWEVER, NOTE THAT TRAINING WILL TAKE A LONG TIME\n",
    "num_train_ag_news = int(len(train_dataset_ag_news) * 0.05)\n",
    "split_train_ag_news_, _ = random_split(split_train_ag_news_, [num_train_ag_news, len(split_train_ag_news_) - num_train_ag_news])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "def label_pipeline(x):\n",
    "   return int(x) - 1\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch_ag_news(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader_ag_news = DataLoader(\n",
    "    split_train_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "valid_dataloader_ag_news = DataLoader(\n",
    "    split_valid_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "test_dataloader_ag_news = DataLoader(\n",
    "    test_dataset_ag_news, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "\n",
    "\n",
    "model_ag_news = Net(num_class=4,vocab_size=vocab_size).to(device)\n",
    "model_ag_news.to(device)\n",
    "\n",
    "\n",
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_ag_news.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_AG News small1.pth\"\n",
    "train_model(model=model_ag_news, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader_ag_news, valid_dataloader=valid_dataloader_ag_news,  epochs=2, save_dir=save_dir, file_name=file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the model you trained earlier with just 2 epochs and a reduced dataset would not be used. Instead, you’ll use a model that has been pretrained using the same method but on the full AG News dataset and with 100 epochs.\n",
    "\n",
    "The following code plots the cost and validation data accuracy for each epoch of the pretrained model up to and including the epoch that yielded the highest accuracy. As you can see, the pretrained model achieved a very high accuracy of over 90% on the AG News validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bQk8mJu3Uct3I4JEsEtRnw/model-AG%20News%20small1-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KNQkqJWWwY_XfbFBRFhZNA/model-AG%20News%20small1-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the pretrained model and evaluates its performance on the AG News test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_ag_news_ = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_ag_news_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "evaluate(test_dataloader_ag_news, model_ag_news_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the pretrained model worked extremely well on the AG News dataset. However, can this model be fine-tuned to perform well on the IMDB dataset as well? Let's find out! You can start off by loading the pretrained AG News model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_fine1 = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_fine1.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB dataset is a binary classification task with only two classes (positive and negative reviews). Therefore, the output layer of the AG NEWS model should be adjusted to have just two output neurons to reflect the binary nature of the IMDB dataset. This adjustment is essential for the model to accurately learn and predict the sentiment of movie reviews in the IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine1.classifier\n",
    "in_features = model_fine1.classifier.in_features\n",
    "print(\"Original final layer:\", model_fine1.classifier)\n",
    "print(\"Input dimention  final layer:\", in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the final layer into a two-class problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine1.classifier = nn.Linear(in_features, 2)\n",
    "model_fine1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the layers that are frozen (`requires_grad == False`) and unfrozen (`requires_grad == True`) in the model. The unfrozen layers will have their weights updated during fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_fine1.named_parameters():\n",
    "    print(f\"{name} requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block simulates fine-tuning on the shortened training set for just 2 epochs. This code should take approximately 2 minutes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_fine1.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_fine1.pth\"\n",
    "train_model(model=model_fine1, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you will not use the model you just fine-tuned on 2 epochs and a shortened dataset. Instead, show the progress of the fine-tuning of the full IMDB training set for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/3LEJw8BRgJJFGqlLxaETxA/model-fine1-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/-CT1h97vjv0TolY82Nw29g/model-fine1-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line loads the prefine-tuned model and evaluates its performance on the IMDB test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/e0WOHKh5dnrbC2lGhpsMMw/model-fine1.pth')\n",
    "model_fine1_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_fine1_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "evaluate(test_dataloader, model_fine1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model demonstrated notable improvement, exhibiting a remarkable achievement with an accuracy of 86% on the test data. This is higher than the 83% achieved by the model trained from scratch on the IMDB dataset. Although the training process was time-intensive (in fact, the fine-tuning was as time-intensive as training the model from scratch), the enhanced performance underscores the fine-tuned model's effectiveness and superiority over the model trained from scratch. A substantial portion of the computational effort was devoted to updating the transformer layers. To expedite the training process, one viable strategy is to focus on training the final layer only, which can significantly reduce the computational load but might compromise the model's accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the final layer only\n",
    "\n",
    "Fine-tuning the final output layer of a neural network is similar to fine-tuning the whole model. You can begin by loading the pretrained model you would like to fine-tune. In this case, it is the same model pretrained on the AG News dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_fine2 = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_fine2.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the key difference: You iterate through all of the parameters in the `model_fine2` model and set the `requires_grad` attribute of each parameter to `False`. This effectively freezes all of the layers in the model, meaning that their weights are be updated during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in the model\n",
    "for param in model_fine2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the final layer to reflect the fact that you are solving a two-class problem. Note that the new layer will be unfrozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=model_fine2.classifier.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine2.classifier = nn.Linear(dim, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine2.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate training the model, simply execute the following cell, which trains on the shorted IMDB train set for just 2 epochs. The following code should take a shorter amount of time to train than the full fine-tuning conducted previously because only the final layer is unfrozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_fine2.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_fine2.pth\"\n",
    "train_model(model=model_fine2, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you will not use the model that you just fine-tuned, but instead inspect the fine-tuning process of a model fine-tuned on the full IMDB training set for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/UdR3ApQnxSeV2mrA0CbiLg/model-fine2-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rWGDIF-uL2dEngWcIo9teQ/model-fine2-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line loads the model fine-tuned for 100 epochs on the full IMDB train set and evaluates its performance on the IMDB test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/B-1H6lpDg-A0zRwpB6Ek2g/model-fine2.pth')\n",
    "model_fine2_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_fine2_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "evaluate(test_dataloader, model_fine2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code indicates that although fine-tuning the final layer takes a significantly smaller amount of time than fine-tuning the whole model, the performance of the model with just the last layer unfrozen is significantly worse (64% accuracy) than the fine-tuned model with all layers unfrozen (86% accuracy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise: Unfreeze specific layers for fine-tuning\n",
    "\n",
    "First, run the following code block to initialize a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetune_some_layers = Net(num_class=4,vocab_size=vocab_size).to(device)\n",
    "\n",
    "# Freeze all layers in the model\n",
    "for param in model_finetune_some_layers.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_finetune_some_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, replace the section labeled `### REPLACE THIS ###` in the following code block to unfreeze the following layers for fine-tuning:\n",
    "- `linear2` for all `layers` in `transformer_encoder`\n",
    "- The `classifier` layer, which you will convert into a layer that has three output classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPLACE THIS ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "# Unfreeze the \"linear2\" layers:\n",
    "for i in range(2):\n",
    "    for param in model_finetune_some_layers.transformer_encoder.layers[i].linear2.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Replace the \"classifier\" layer. Note that this has the effect of unfreezing that layer:\n",
    "model_finetune_some_layers.classifier  = nn.Linear(100, 3)\n",
    "\n",
    "# Output all parameters indicating whether those parameters are unfrozen:\n",
    "for name, param in model_finetune_some_layers.named_parameters():\n",
    "    print(f\"{name} requires_grad: {param.requires_grad}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo)\n",
    "\n",
    "Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wojciech \"Victor\" Fulmyk](https://www.linkedin.com/in/wfulmyk) \n",
    "\n",
    "Wojciech \"Victor\" Fulmyk is a Data Scientist at IBM, and a PhD Candidate in economics at the University of Calgary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS from Dalhousie University. He has previous experience working with Natural Language Processing and as a Data Scientist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "[TEXT CLASSIFICATION WITH THE TORCHTEXT LIBRARY](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "c3d83d314ee5c88f2e45e07f3cf0feb0249ea1b26aa9e043916e4870ce71dd34"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
